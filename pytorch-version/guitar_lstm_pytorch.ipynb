{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "from torch import nn\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A model with the same name already exists. Please choose a new name.\n"
     ]
    }
   ],
   "source": [
    "# EDIT THIS SECTION FOR USER INPUTS\n",
    "#\n",
    "name = 'test'\n",
    "in_file = 'data/ts9_test1_in_FP32.wav'\n",
    "out_file = 'data/ts9_test1_out_FP32.wav'\n",
    "epochs = 1\n",
    "\n",
    "train_mode = 0     # 0 = speed training, \n",
    "                   # 1 = accuracy training \n",
    "                   # 2 = extended training\n",
    "\n",
    "input_size = 150 \n",
    "batch_size = 4096 \n",
    "test_size = 0.2\n",
    "\n",
    "if not os.path.exists('models/'+name):\n",
    "    os.makedirs('models/'+name)\n",
    "else:\n",
    "    print(\"A model with the same name already exists. Please choose a new name.\")\n",
    "    exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    wavfile.write(name, 44100, data.flatten().astype(np.float32))\n",
    "\n",
    "def normalize(data):\n",
    "    data_max = max(data)\n",
    "    data_min = min(data)\n",
    "    data_norm = max(data_max,abs(data_min))\n",
    "    return data / data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_training shape: (6587907,)\n",
      "X_training shape: (6587907,)\n",
      "y_ordered_training shape: torch.Size([6587758])\n",
      "X_ordered_training shape: torch.Size([6587758, 150])\n"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess Data ###########################################\n",
    "in_rate, in_data = wavfile.read(in_file)\n",
    "out_rate, out_data = wavfile.read(out_file)\n",
    "\n",
    "X_all = in_data.astype(np.float32).flatten()  \n",
    "y_all = out_data.astype(np.float32).flatten() \n",
    "\n",
    "# Get the last 20% of the wav data for testing and thee rest for training\n",
    "y_training, y_testing = np.split(y_all, [int(len(y_all)*.8)])\n",
    "X_training, X_testing = np.split(X_all, [int(len(X_all)*.8)])\n",
    "print(f\"y_training shape: {y_training.shape}\")\n",
    "print(f\"X_training shape: {X_training.shape}\")\n",
    "\n",
    "# The input size defines the number of samples used for each prediction\n",
    "# Therefore the first output value that we get is at index input_size-1\n",
    "y_ordered_training = y_training[input_size-1:]\n",
    "y_ordered_training = torch.from_numpy(y_ordered_training)\n",
    "print(f\"y_ordered_training shape: {y_ordered_training.shape}\")\n",
    "\n",
    "indices = np.arange(input_size) + np.arange(len(X_training)-input_size+1)[:,np.newaxis]\n",
    "indices = torch.from_numpy(indices)\n",
    "X_training = torch.from_numpy(X_training)\n",
    "X_ordered_training = torch.zeros_like(indices, dtype=torch.float32)\n",
    "for i, j in enumerate(indices):\n",
    "    X_ordered_training[i] = torch.gather(X_training, 0, indices[i])\n",
    "    \n",
    "print(f\"X_ordered_training shape: {X_ordered_training.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Shape of X [N, C, H, W]: torch.Size([4096, 150])\n",
      "Shape of y: torch.Size([4096]) torch.float32\n",
      "This is the first audio chunk in the batch: tensor([ 0.0328,  0.0392,  0.0453,  0.0513,  0.0569,  0.0623,  0.0674,  0.0722,\n",
      "         0.0766,  0.0807,  0.0844,  0.0878,  0.0908,  0.0934,  0.0956,  0.0975,\n",
      "         0.0989,  0.1000,  0.1007,  0.1010,  0.1011,  0.1008,  0.1002,  0.0995,\n",
      "         0.0985,  0.0974,  0.0959,  0.0944,  0.0926,  0.0907,  0.0887,  0.0865,\n",
      "         0.0844,  0.0822,  0.0798,  0.0775,  0.0750,  0.0725,  0.0700,  0.0675,\n",
      "         0.0650,  0.0625,  0.0601,  0.0577,  0.0554,  0.0531,  0.0509,  0.0487,\n",
      "         0.0465,  0.0444,  0.0424,  0.0404,  0.0385,  0.0366,  0.0349,  0.0332,\n",
      "         0.0316,  0.0302,  0.0287,  0.0274,  0.0261,  0.0249,  0.0236,  0.0224,\n",
      "         0.0212,  0.0200,  0.0188,  0.0177,  0.0165,  0.0155,  0.0145,  0.0135,\n",
      "         0.0126,  0.0117,  0.0110,  0.0103,  0.0096,  0.0088,  0.0081,  0.0074,\n",
      "         0.0067,  0.0060,  0.0053,  0.0046,  0.0039,  0.0033,  0.0026,  0.0020,\n",
      "         0.0014,  0.0009,  0.0004, -0.0002, -0.0007, -0.0013, -0.0018, -0.0023,\n",
      "        -0.0028, -0.0034, -0.0039, -0.0044, -0.0049, -0.0053, -0.0056, -0.0060,\n",
      "        -0.0062, -0.0064, -0.0066, -0.0067, -0.0067, -0.0068, -0.0068, -0.0068,\n",
      "        -0.0068, -0.0067, -0.0065, -0.0063, -0.0061, -0.0059, -0.0055, -0.0051,\n",
      "        -0.0046, -0.0040, -0.0033, -0.0025, -0.0018, -0.0011, -0.0003,  0.0006,\n",
      "         0.0014,  0.0022,  0.0031,  0.0039,  0.0049,  0.0058,  0.0067,  0.0074,\n",
      "         0.0082,  0.0090,  0.0100,  0.0110,  0.0119,  0.0125,  0.0130,  0.0134,\n",
      "         0.0140,  0.0146,  0.0154,  0.0163,  0.0171,  0.0176])\n",
      "This is the first target value in the batch: -0.11651611328125\n",
      "y_ordered_testing shape: torch.Size([1646828])\n",
      "Batch: 0\n",
      "Shape of X [N, C, H, W]: torch.Size([4096, 150])\n",
      "Shape of y: torch.Size([4096]) torch.float32\n",
      "This is the first audio chunk in the batch: tensor([-6.4392e-03, -5.5542e-03, -4.7302e-03, -3.6011e-03, -2.0142e-03,\n",
      "         1.2207e-04,  2.5024e-03,  4.6387e-03,  5.8594e-03,  5.6458e-03,\n",
      "         4.2725e-03,  2.1667e-03,  3.0518e-05, -1.7090e-03, -3.1128e-03,\n",
      "        -4.3030e-03, -5.1575e-03, -5.2490e-03, -3.9978e-03, -1.4954e-03,\n",
      "         1.6479e-03,  4.6082e-03,  6.8665e-03,  8.0872e-03,  8.2397e-03,\n",
      "         7.4158e-03,  5.6763e-03,  2.8381e-03, -1.0376e-03, -5.4626e-03,\n",
      "        -9.9792e-03, -1.4374e-02, -1.8219e-02, -2.1027e-02, -2.2827e-02,\n",
      "        -2.3895e-02, -2.4292e-02, -2.4139e-02, -2.3773e-02, -2.3346e-02,\n",
      "        -2.2797e-02, -2.2369e-02, -2.2400e-02, -2.2949e-02, -2.3376e-02,\n",
      "        -2.2858e-02, -2.0721e-02, -1.6876e-02, -1.1658e-02, -5.9509e-03,\n",
      "        -4.2725e-04,  4.7607e-03,  9.5215e-03,  1.3489e-02,  1.5930e-02,\n",
      "         1.5808e-02,  1.2939e-02,  8.0872e-03,  2.4414e-03, -2.7771e-03,\n",
      "        -6.8359e-03, -9.4604e-03, -1.0406e-02, -9.5215e-03, -6.8054e-03,\n",
      "        -2.1362e-03,  4.3640e-03,  1.2054e-02,  2.0172e-02,  2.8137e-02,\n",
      "         3.5431e-02,  4.1168e-02,  4.5044e-02,  4.7241e-02,  4.7913e-02,\n",
      "         4.6753e-02,  4.3884e-02,  3.9642e-02,  3.4485e-02,  2.8595e-02,\n",
      "         2.2614e-02,  1.7120e-02,  1.2115e-02,  7.2632e-03,  2.5330e-03,\n",
      "        -2.2583e-03, -6.8665e-03, -1.0529e-02, -1.3184e-02, -1.5045e-02,\n",
      "        -1.6113e-02, -1.6937e-02, -1.7639e-02, -1.7853e-02, -1.7273e-02,\n",
      "        -1.5900e-02, -1.4038e-02, -1.2054e-02, -1.0742e-02, -1.0132e-02,\n",
      "        -9.5825e-03, -8.9111e-03, -7.8430e-03, -5.7678e-03, -3.1128e-03,\n",
      "        -7.3242e-04,  1.0681e-03,  1.9531e-03,  1.4648e-03,  1.5259e-04,\n",
      "        -1.1597e-03, -2.7466e-03, -4.7913e-03, -6.9580e-03, -8.6670e-03,\n",
      "        -9.8267e-03, -9.7961e-03, -8.1787e-03, -5.5847e-03, -3.0823e-03,\n",
      "        -6.1035e-04,  1.7700e-03,  3.6011e-03,  4.4861e-03,  4.5166e-03,\n",
      "         3.2349e-03,  4.8828e-04, -3.2043e-03, -7.2632e-03, -1.1658e-02,\n",
      "        -1.5961e-02, -1.9928e-02, -2.3254e-02, -2.5604e-02, -2.6978e-02,\n",
      "        -2.7771e-02, -2.7924e-02, -2.7405e-02, -2.6398e-02, -2.5543e-02,\n",
      "        -2.5269e-02, -2.5757e-02, -2.6794e-02, -2.7588e-02, -2.7039e-02,\n",
      "        -2.4841e-02, -2.1362e-02, -1.6998e-02, -1.2054e-02, -6.7749e-03])\n",
      "This is the first target value in the batch: 0.0400390625\n"
     ]
    }
   ],
   "source": [
    "training_dataset = torch.utils.data.TensorDataset(X_ordered_training, y_ordered_training)\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch, (X, y) in enumerate(training_dataloader):\n",
    "    print(f\"Batch: {batch}\")\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    print(f\"This is the first audio chunk in the batch: {X[0]}\")\n",
    "    print(f\"This is the first target value in the batch: {y[0]}\")\n",
    "    break\n",
    "\n",
    "# The input size defines the number of samples used for each prediction\n",
    "# Therefore the first output value that we get is at index input_size-1\n",
    "y_ordered_testing = y_testing[input_size-1:]\n",
    "y_ordered_testing = torch.from_numpy(y_ordered_testing)\n",
    "print(f\"y_ordered_testing shape: {y_ordered_testing.shape}\")\n",
    "\n",
    "indices_testing = np.arange(input_size) + np.arange(len(X_testing)-input_size+1)[:,np.newaxis]\n",
    "indices_testing = torch.from_numpy(indices_testing)\n",
    "X_testing = torch.from_numpy(X_testing)\n",
    "X_ordered_testing = torch.zeros_like(indices_testing, dtype=torch.float32)\n",
    "for i, j in enumerate(indices_testing):\n",
    "    X_ordered_testing[i] = torch.gather(X_testing, 0, indices[i])\n",
    "\n",
    "testing_dataset = torch.utils.data.TensorDataset(X_ordered_testing, y_ordered_testing)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=batch_size)\n",
    "\n",
    "for batch, (X, y) in enumerate(testing_dataloader):\n",
    "    print(f\"Batch: {batch}\")\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    print(f\"This is the first audio chunk in the batch: {X[0]}\")\n",
    "    print(f\"This is the first target value in the batch: {y[0]}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (pad): ConstantPad1d(padding=(12, 12), value=0)\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(12,), stride=(12,))\n",
      "  (conv2): Conv1d(16, 16, kernel_size=(12,), stride=(12,))\n",
      "  (lstm): LSTM(3, 36, batch_first=True)\n",
      "  (linear): Linear(in_features=36, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "if train_mode == 0:         # Speed Training\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 12    \n",
    "    conv1d_filters = 16\n",
    "    hidden_units = 36\n",
    "elif train_mode == 1:       # Accuracy Training (~10x longer than Speed Training)\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 4\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 64\n",
    "else:                       # Extended Training (~60x longer than Accuracy Training)\n",
    "    learning_rate = 0.0005 \n",
    "    conv1d_strides = 3\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 96\n",
    "\n",
    "# # Create Sequential Model ###########################################\n",
    "# clear_session()\n",
    "# model = Sequential()\n",
    "# model.add(Conv1D(conv1d_filters, 12,strides=conv1d_strides, activation=None, padding='same',input_shape=(input_size,1)))\n",
    "# model.add(Conv1D(conv1d_filters, 12,strides=conv1d_strides, activation=None, padding='same'))\n",
    "# model.add(LSTM(hidden_units))\n",
    "# model.add(Dense(1, activation=None))\n",
    "# model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=[error_to_signal])\n",
    "# model.summary()\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ConstantPad1d(padding=12, value=0)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=conv1d_filters, kernel_size=12, stride=conv1d_strides) # Padding needed\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv1d_filters, out_channels=conv1d_filters, kernel_size=12, stride=conv1d_strides) # Padding needed\n",
    "        self.lstm = nn.LSTM(input_size=3, hidden_size=hidden_units, batch_first = True)\n",
    "        self.linear = nn.Linear(in_features=hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.conv2(x)\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        x = self.linear(cell)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X = X.unsqueeze(1)\n",
    "        y = y.unsqueeze(0)\n",
    "        y = y.unsqueeze(2)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            X = X.unsqueeze(1)\n",
    "            y = y.unsqueeze(0)\n",
    "            y = y.unsqueeze(2)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.055807  [ 4096/6587758]\n",
      "loss: 0.001651  [413696/6587758]\n",
      "loss: 0.000495  [823296/6587758]\n",
      "loss: 0.000321  [1232896/6587758]\n",
      "loss: 0.000239  [1642496/6587758]\n",
      "loss: 0.000186  [2052096/6587758]\n",
      "loss: 0.000181  [2461696/6587758]\n",
      "loss: 0.000162  [2871296/6587758]\n",
      "loss: 0.000153  [3280896/6587758]\n",
      "loss: 0.000141  [3690496/6587758]\n",
      "loss: 0.000129  [4100096/6587758]\n",
      "loss: 0.000118  [4509696/6587758]\n",
      "loss: 0.000114  [4919296/6587758]\n",
      "loss: 0.000119  [5328896/6587758]\n",
      "loss: 0.000107  [5738496/6587758]\n",
      "loss: 0.000104  [6148096/6587758]\n",
      "loss: 0.000113  [6557696/6587758]\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 0.000096 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 1 \n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_dataloader, model, loss_fn, optimizer)\n",
    "    test(testing_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"models/\"+name+\"/model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"models/\"+name+\"/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.zeros(0).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, _ in testing_dataloader:\n",
    "        X = X.to(device)\n",
    "        X = X.unsqueeze(1)\n",
    "        pred = model(X)\n",
    "        result = torch.cat((result, pred.flatten()), 0)\n",
    "save_wav('models/'+name+'/result.wav', result.cpu().numpy())\n",
    "save_wav('models/'+name+'/input.wav', X_testing.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
