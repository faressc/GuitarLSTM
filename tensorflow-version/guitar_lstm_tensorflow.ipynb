{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tf2onnx\n",
    "import onnx\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose computation device (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the physical devices available:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "These are the visible devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(f\"These are the physical devices available:\\n{physical_devices}\")\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    print(f\"These are the visible devices:\\n{visible_devices}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A model with the same name already exists. Please choose a new name.\n"
     ]
    }
   ],
   "source": [
    "# EDIT THIS SECTION FOR USER INPUTS\n",
    "#\n",
    "name = 'model_0'\n",
    "in_file = '../data/ts9_test1_in_FP32.wav'\n",
    "out_file = '../data/ts9_test1_out_FP32.wav'\n",
    "epochs = 1\n",
    "\n",
    "train_mode = 0     # 0 = speed training, \n",
    "                   # 1 = accuracy training \n",
    "                   # 2 = extended training\n",
    "\n",
    "input_size = 150 \n",
    "batch_size = 4096 \n",
    "test_size = 0.2\n",
    "\n",
    "if not os.path.exists('models/'+name):\n",
    "    os.makedirs('models/'+name)\n",
    "else:\n",
    "    print(\"A model with the same name already exists. Please choose a new name.\")\n",
    "    exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    sp.io.wavfile.write(name, 44100, data.flatten().astype(np.float32))\n",
    "\n",
    "def normalize(data):\n",
    "    data_max = max(data)\n",
    "    data_min = min(data)\n",
    "    data_norm = max(data_max,abs(data_min))\n",
    "    return data / data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training shape (pre-processing): (6587907, 1)\n",
      "y_training shape (pre-processing): (6587907, 1)\n",
      "X_testing shape (pre-processing): (1646977, 1)\n",
      "y_testing shape (pre-processing): (1646977, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 13:58:20.534818: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3952654800 exceeds 10% of free system memory.\n",
      "2024-03-08 13:58:21.155171: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1976327400 exceeds 10% of free system memory.\n",
      "2024-03-08 13:58:21.625643: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3952654800 exceeds 10% of free system memory.\n",
      "2024-03-08 13:58:22.294189: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1976327400 exceeds 10% of free system memory.\n",
      "2024-03-08 13:58:22.753254: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3952654800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_ordered_training shape: (6587758, 150, 1)\n",
      "X_ordered_testing shape: (1646828, 150, 1)\n",
      "y_ordered_training shape: (6587758, 1)\n",
      "y_ordered_testing shape: (1646828, 1)\n",
      "X_random_training shape (post-processing): (6587758, 150, 1)\n",
      "y_random_training shape (post-processing): (6587758, 1)\n",
      "The X_random_training data is an array, where each element is an array of input_size samples in time order. Therefore the lenght is smaller than the original X_training array (the first 150 samples are grouped).\n",
      "The y_random_training data is an array, where each element is a single sample. This single sample is the target output for the corresponding X_random_training element, which consists of input samples.\n"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess Data ###########################################\n",
    "in_rate, in_data = sp.io.wavfile.read(in_file)\n",
    "out_rate, out_data = sp.io.wavfile.read(out_file)\n",
    "\n",
    "X_all = in_data.astype(np.float32).flatten()  \n",
    "X_all = normalize(X_all).reshape(len(X_all),1)   \n",
    "y_all = out_data.astype(np.float32).flatten() \n",
    "y_all = normalize(y_all).reshape(len(y_all),1)\n",
    "\n",
    "# Get the last 20% of the wav data for testing and thee rest for training\n",
    "X_training, X_testing = np.split(X_all, [int(len(X_all)*(1-test_size))])\n",
    "y_training, y_testing = np.split(y_all, [int(len(y_all)*(1-test_size))])\n",
    "print(f\"X_training shape (pre-processing): {X_training.shape}\")\n",
    "print(f\"y_training shape (pre-processing): {y_training.shape}\")\n",
    "print(f\"X_testing shape (pre-processing): {X_testing.shape}\")\n",
    "print(f\"y_testing shape (pre-processing): {y_testing.shape}\")\n",
    "\n",
    "# Create a new array where each element is an array of input_size samples in time order\n",
    "# Each element of the new array is shifted by one sample from the previous element\n",
    "indices = np.arange(input_size) + np.arange(len(X_training)-input_size+1)[:,np.newaxis]\n",
    "\n",
    "# We need to split the data otherwise the tf.gather function will throw an error since we run out of memory\n",
    "X_ordered_training = tf.gather(X_training,indices[:len(indices)//2])\n",
    "X_ordered_training = tf.concat([X_ordered_training, tf.gather(X_training,indices[len(indices)//2:])], axis=0)\n",
    "print(f\"X_ordered_training shape: {X_ordered_training.shape}\")\n",
    "\n",
    "indices = np.arange(input_size) + np.arange(len(X_testing)-input_size+1)[:,np.newaxis]\n",
    "X_ordered_testing = tf.gather(X_testing,indices) \n",
    "print(f\"X_ordered_testing shape: {X_ordered_testing.shape}\")\n",
    "\n",
    "# The input size defines the number of samples used for each prediction\n",
    "# Therefore the first output value that we get is at index input_size-1\n",
    "y_ordered_training = y_training[input_size-1:]\n",
    "print(f\"y_ordered_training shape: {y_ordered_training.shape}\")\n",
    "y_ordered_testing = y_testing[input_size-1:]\n",
    "print(f\"y_ordered_testing shape: {y_ordered_testing.shape}\")\n",
    "\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X_ordered_training)) \n",
    "X_random_training = tf.gather(X_ordered_training, shuffled_indices)\n",
    "y_random_training = tf.gather(y_ordered_training, shuffled_indices)\n",
    "print(f\"X_random_training shape (post-processing): {X_random_training.shape}\")\n",
    "print(f\"y_random_training shape (post-processing): {y_random_training.shape}\")\n",
    "\n",
    "print(f\"The X_random_training data is an array, where each element is an array of input_size samples in time order. Therefore the lenght is smaller than the original X_training array (the first {input_size} samples are grouped).\")\n",
    "print(f\"The y_random_training data is an array, where each element is a single sample. This single sample is the target output for the corresponding X_random_training element, which consists of input samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"neural_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             multiple                  208       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           multiple                  3088      \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  7632      \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  37        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10965 (42.83 KB)\n",
      "Trainable params: 10965 (42.83 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''This is a similar Tensorflow/Keras implementation of the LSTM model from the paper:\n",
    "    \"Real-Time Guitar Amplifier Emulation with Deep Learning\"\n",
    "    https://www.mdpi.com/2076-3417/10/3/766/htm\n",
    "\n",
    "    Uses a stack of two 1-D Convolutional layers, followed by LSTM, followed by \n",
    "    a Dense (fully connected) layer. Three preset training modes are available, \n",
    "    with further customization by editing the code. A Functional keras model \n",
    "    is implemented here.\n",
    "\n",
    "    Note: RAM may be a limiting factor for the parameter \"input_size\". The wav data\n",
    "      is preprocessed and stored in RAM, which improves training speed but quickly runs out\n",
    "      if using a large number for \"input_size\".  Reduce this if you are experiencing\n",
    "      RAM issues.\n",
    "'''\n",
    "\n",
    "if train_mode == 0:         # Speed Training\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 12   \n",
    "    conv1d_1_strides = 12\n",
    "    conv1d_filters = 16\n",
    "    hidden_units = 36\n",
    "elif train_mode == 1:       # Accuracy Training (~10x longer than Speed Training)\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 4\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 64\n",
    "else:                       # Extended Training (~60x longer than Accuracy Training)\n",
    "    learning_rate = 0.0005 \n",
    "    conv1d_strides = 3\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 96\n",
    "\n",
    "# Create Functional Model ###########################################\n",
    "keras.backend.clear_session()\n",
    "\n",
    "class NeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_size, conv1d_filters, conv1d_strides, hidden_units, learning_rate):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.conv1d_1 = keras.layers.Conv1D(filters=conv1d_filters, kernel_size=12, strides=conv1d_strides, activation=None, batch_size=1)\n",
    "        self.conv1d_2 = keras.layers.Conv1D(filters=conv1d_filters, kernel_size=12, strides=conv1d_strides, activation=None, batch_size=1)\n",
    "        self.lstm = keras.layers.LSTM(units=hidden_units, return_sequences=False, stateful=False, batch_size=batch_size, kernel_initializer='glorot_uniform', recurrent_initializer='zeros', bias_initializer='zeros', use_bias=True)\n",
    "        self.dense = keras.layers.Dense(units=1, activation=None, batch_size=1)\n",
    "\n",
    "        self.build((1, input_size, 1))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = keras.layers.ZeroPadding1D(padding=12, batch_size=1)(inputs)\n",
    "        x = self.conv1d_1(x)\n",
    "        x = keras.layers.ZeroPadding1D(padding=12, batch_size=1)(x)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.lstm(x)\n",
    "        return self.dense(x)\n",
    "\n",
    "# Define the input shape\n",
    "inputs = keras.Input(shape=(input_size,1), batch_size=1)\n",
    "\n",
    "model = NeuralNetwork(input_size, conv1d_filters, conv1d_strides, hidden_units, learning_rate)\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609/1609 [==============================] - 43s 25ms/step - loss: 8.0674e-04 - val_loss: 3.0798e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_random_training, y=y_random_training, epochs=epochs, batch_size=batch_size, validation_data=(X_ordered_testing, y_ordered_testing)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('models/'+name+'/'+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions\n",
    "### 0. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7c012c738590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('models/'+name+'/'+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. On the test audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction..\n",
      "51464/51464 [==============================] - 67s 1ms/step\n",
      "X_testing shape:  (1646977, 1)\n",
      "X_ordered_testing shape:  (1646828, 150, 1)\n",
      "y_testing shape:  (1646977, 1)\n",
      "prediction shape:  (1646828, 1)\n",
      "Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Prediction #################################################\n",
    "# Test the model on the testing data #############################\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "prediction = model.predict(X_ordered_testing)\n",
    "\n",
    "save_wav('models/'+name+'/y_pred.wav', prediction)\n",
    "save_wav('models/'+name+'/x_test.wav', X_testing)\n",
    "save_wav('models/'+name+'/y_test.wav', y_testing)\n",
    "\n",
    "print(\"X_testing shape: \", X_testing.shape)\n",
    "print(\"X_ordered_testing shape: \", X_ordered_testing.shape)\n",
    "print(\"y_testing shape: \", y_testing.shape)\n",
    "print(\"prediction shape: \", prediction.shape)\n",
    "\n",
    "print(\"Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. On a number sequence (to control inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction..\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "prediction [[-0.15528308]\n",
      " [-0.2584993 ]]\n",
      "X_testing_2 shape:  (2, 150, 1)\n",
      "prediction_2 shape:  (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the model simple number sequence to compare with inference #\n",
    "X_testing_2 = np.array([])\n",
    "\n",
    "for i in range(0, 300):\n",
    "    X_testing_2 = np.append(X_testing_2, i*0.001)\n",
    "\n",
    "X_testing_2 = np.expand_dims(X_testing_2, axis=0)\n",
    "X_testing_2 = np.expand_dims(X_testing_2, axis=0)\n",
    "\n",
    "X_testing_2 = np.reshape(X_testing_2, (2, 150, 1))\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "prediction_2 = model.predict(X_testing_2)\n",
    "print(f\"prediction {prediction_2}\")\n",
    "\n",
    "print(\"X_testing_2 shape: \", X_testing_2.shape)\n",
    "print(\"prediction_2 shape: \", prediction_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpf_ywq6r3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpf_ywq6r3/assets\n",
      "2024-03-08 14:16:14.984963: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-08 14:16:14.984983: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-08 14:16:14.985126: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpf_ywq6r3\n",
      "2024-03-08 14:16:14.990758: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-08 14:16:14.990768: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpf_ywq6r3\n",
      "2024-03-08 14:16:15.006423: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-08 14:16:15.070747: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpf_ywq6r3\n",
      "2024-03-08 14:16:15.109141: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 124014 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 28, Total Ops 43, % non-converted = 65.12 %\n",
      " * 28 ARITH ops\n",
      "\n",
      "- arith.constant:   28 occurrences  (f32: 20, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "2024-03-08 14:16:15.281752: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 1.546 M  ops, equivalently 0.773 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpyy4kmsej/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpyy4kmsej/assets\n",
      "2024-03-08 14:16:27.492635: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-08 14:16:27.492660: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-08 14:16:27.492839: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpyy4kmsej\n",
      "2024-03-08 14:16:27.498338: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-08 14:16:27.498351: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpyy4kmsej\n",
      "2024-03-08 14:16:27.515862: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-08 14:16:27.582341: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpyy4kmsej\n",
      "2024-03-08 14:16:27.623338: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 130500 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 28, Total Ops 43, % non-converted = 65.12 %\n",
      " * 28 ARITH ops\n",
      "\n",
      "- arith.constant:   28 occurrences  (f32: 20, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "2024-03-08 14:16:27.779591: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 3.092 M  ops, equivalently 1.546 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7cgp5vgp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp7cgp5vgp/assets\n",
      "2024-03-08 14:16:40.395618: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-08 14:16:40.395637: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-08 14:16:40.395874: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp7cgp5vgp\n",
      "2024-03-08 14:16:40.401983: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-08 14:16:40.402000: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp7cgp5vgp\n",
      "2024-03-08 14:16:40.419648: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-08 14:16:40.485397: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp7cgp5vgp\n",
      "2024-03-08 14:16:40.525313: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 129439 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 28, Total Ops 43, % non-converted = 65.12 %\n",
      " * 28 ARITH ops\n",
      "\n",
      "- arith.constant:   28 occurrences  (f32: 20, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "2024-03-08 14:16:40.688074: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 6.183 M  ops, equivalently 3.092 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4aaw_96q/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4aaw_96q/assets\n",
      "2024-03-08 14:16:53.115003: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-08 14:16:53.115020: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-08 14:16:53.115177: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp4aaw_96q\n",
      "2024-03-08 14:16:53.120812: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-08 14:16:53.120824: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp4aaw_96q\n",
      "2024-03-08 14:16:53.137079: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-08 14:16:53.201756: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp4aaw_96q\n",
      "2024-03-08 14:16:53.240697: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 125520 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 28, Total Ops 43, % non-converted = 65.12 %\n",
      " * 28 ARITH ops\n",
      "\n",
      "- arith.constant:   28 occurrences  (f32: 20, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "2024-03-08 14:16:53.387754: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 12.366 M  ops, equivalently 6.183 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpctl9rbb5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpctl9rbb5/assets\n",
      "2024-03-08 14:17:05.699908: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-08 14:17:05.699928: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-08 14:17:05.700069: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpctl9rbb5\n",
      "2024-03-08 14:17:05.705666: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-08 14:17:05.705678: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpctl9rbb5\n",
      "2024-03-08 14:17:05.722090: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-08 14:17:05.785956: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpctl9rbb5\n",
      "2024-03-08 14:17:05.824196: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 124127 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 28, Total Ops 43, % non-converted = 65.12 %\n",
      " * 28 ARITH ops\n",
      "\n",
      "- arith.constant:   28 occurrences  (f32: 20, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "2024-03-08 14:17:05.971255: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 24.733 M  ops, equivalently 12.366 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6ns0_a_v/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp6ns0_a_v/assets\n",
      "2024-03-08 14:17:18.221699: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-08 14:17:18.221719: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-08 14:17:18.221860: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp6ns0_a_v\n",
      "2024-03-08 14:17:18.227178: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-08 14:17:18.227188: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp6ns0_a_v\n",
      "2024-03-08 14:17:18.242415: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-08 14:17:18.306527: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmp6ns0_a_v\n",
      "2024-03-08 14:17:18.344425: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 122565 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 28, Total Ops 43, % non-converted = 65.12 %\n",
      " * 28 ARITH ops\n",
      "\n",
      "- arith.constant:   28 occurrences  (f32: 20, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "2024-03-08 14:17:18.493825: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 49.465 M  ops, equivalently 24.733 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpsz7qm609/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpsz7qm609/assets\n",
      "2024-03-08 14:17:30.562638: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-08 14:17:30.562657: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-08 14:17:30.562804: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpsz7qm609\n",
      "2024-03-08 14:17:30.568282: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-08 14:17:30.568293: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpsz7qm609\n",
      "2024-03-08 14:17:30.584497: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-08 14:17:30.648224: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpsz7qm609\n",
      "2024-03-08 14:17:30.686734: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 123930 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 28, Total Ops 43, % non-converted = 65.12 %\n",
      " * 28 ARITH ops\n",
      "\n",
      "- arith.constant:   28 occurrences  (f32: 20, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "2024-03-08 14:17:30.839028: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 98.931 M  ops, equivalently 49.465 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkoxhytga/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkoxhytga/assets\n",
      "2024-03-08 14:17:43.311805: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-03-08 14:17:43.311826: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-03-08 14:17:43.311968: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpkoxhytga\n",
      "2024-03-08 14:17:43.317513: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-03-08 14:17:43.317525: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpkoxhytga\n",
      "2024-03-08 14:17:43.333436: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-03-08 14:17:43.397171: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpkoxhytga\n",
      "2024-03-08 14:17:43.435825: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 123857 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 28, Total Ops 43, % non-converted = 65.12 %\n",
      " * 28 ARITH ops\n",
      "\n",
      "- arith.constant:   28 occurrences  (f32: 20, i32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 1)\n",
      "\n",
      "  (f32: 2)\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n",
      "2024-03-08 14:17:43.592208: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2989] Estimated count of arithmetic ops: 197.861 M  ops, equivalently 98.931 M  MACs\n"
     ]
    }
   ],
   "source": [
    "# tflite does not support dynamic batch sizes, so we need to create a model for each batch size\n",
    "\n",
    "buffer_sizes = [64, 128, 256, 512, 1024, 2048, 4096, 8192]\n",
    "model_input_size = 150\n",
    "# rf is the receptive field and hence the kernel size of the last layer because of the dilation\n",
    "# hence the model input size needs to be at least rf\n",
    "for buffer_size in buffer_sizes:\n",
    "    batch_size = buffer_size\n",
    "    input_shape = [batch_size, model_input_size, 1]\n",
    "\n",
    "    func = tf.function(model).get_concrete_function(\n",
    "        tf.TensorSpec(input_shape, dtype=tf.float32))\n",
    "    converter = tf.lite.TFLiteConverter.from_concrete_functions([func], model)\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Save the model.\n",
    "    with open(\"models/\"+name+\"/\"+\"GuitarLSTM-\"+str(buffer_size)+\".tflite\", 'wb') as f:\n",
    "      f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 14:18:22.800893: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:22.800971: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:23.034716: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:23.034799: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:23.695070: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:23.695138: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:24.328572: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:24.328685: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:25.014727: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:25.014813: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:25.262671: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:25.262754: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:25.933882: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:25.933978: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:26.164816: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:26.164907: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:26.828092: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:26.828170: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:27.067165: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:27.067235: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:28.156732: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:28.156843: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:28.398525: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:28.398596: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:29.076626: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:29.076708: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:29.321076: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:29.321157: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:29.998201: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:29.998291: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-03-08 14:18:30.239483: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-03-08 14:18:30.239595: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n"
     ]
    }
   ],
   "source": [
    "for buffer_size in buffer_sizes:\n",
    "    batch_size = buffer_size\n",
    "    input_shape = [batch_size, model_input_size, 1]\n",
    "\n",
    "    # Define the input shape\n",
    "    input_signature = [tf.TensorSpec(input_shape, tf.float32, name='x')]\n",
    "\n",
    "    # Convert the model\n",
    "    onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=18)\n",
    "    onnx.save(proto=onnx_model, f=\"models/\"+name+\"/\"+\"GuitarLSTM-tflite-\"+str(buffer_size)+\".onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
