{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose computation device (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the physical devices available:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "These are the visible devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(f\"These are the physical devices available:\\n{physical_devices}\")\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    print(f\"These are the visible devices:\\n{visible_devices}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A model with the same name already exists. Please choose a new name.\n"
     ]
    }
   ],
   "source": [
    "# EDIT THIS SECTION FOR USER INPUTS\n",
    "#\n",
    "name = 'model_0'\n",
    "in_file = '../data/ts9_test1_in_FP32.wav'\n",
    "out_file = '../data/ts9_test1_out_FP32.wav'\n",
    "epochs = 1\n",
    "\n",
    "train_mode = 0     # 0 = speed training, \n",
    "                   # 1 = accuracy training \n",
    "                   # 2 = extended training\n",
    "\n",
    "input_size = 150 \n",
    "batch_size = 4096 \n",
    "test_size = 0.2\n",
    "\n",
    "if not os.path.exists('models/'+name):\n",
    "    os.makedirs('models/'+name)\n",
    "else:\n",
    "    print(\"A model with the same name already exists. Please choose a new name.\")\n",
    "    exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    sp.io.wavfile.write(name, 44100, data.flatten().astype(np.float32))\n",
    "\n",
    "def normalize(data):\n",
    "    data_max = max(data)\n",
    "    data_min = min(data)\n",
    "    data_norm = max(data_max,abs(data_min))\n",
    "    return data / data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training shape (pre-processing): (6587907, 1)\n",
      "y_training shape (pre-processing): (6587907, 1)\n",
      "X_testing shape (pre-processing): (1646977, 1)\n",
      "y_testing shape (pre-processing): (1646977, 1)\n",
      "X_ordered_training shape: (6587758, 150, 1)\n",
      "X_ordered_testing shape: (1646828, 150, 1)\n",
      "y_ordered_training shape: (6587758, 1)\n",
      "y_ordered_testing shape: (1646828, 1)\n",
      "X_random_training shape (post-processing): (6587758, 150, 1)\n",
      "y_random_training shape (post-processing): (6587758, 1)\n",
      "The X_random_training data is an array, where each element is an array of input_size samples in time order. Therefore the lenght is smaller than the original X_training array (the first 150 samples are grouped).\n",
      "The y_random_training data is an array, where each element is a single sample. This single sample is the target output for the corresponding X_random_training element, which consists of input samples.\n"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess Data ###########################################\n",
    "in_rate, in_data = sp.io.wavfile.read(in_file)\n",
    "out_rate, out_data = sp.io.wavfile.read(out_file)\n",
    "\n",
    "X_all = in_data.astype(np.float32).flatten()  \n",
    "X_all = normalize(X_all).reshape(len(X_all),1)   \n",
    "y_all = out_data.astype(np.float32).flatten() \n",
    "y_all = normalize(y_all).reshape(len(y_all),1)\n",
    "\n",
    "# Get the last 20% of the wav data for testing and thee rest for training\n",
    "X_training, X_testing = np.split(X_all, [int(len(X_all)*(1-test_size))])\n",
    "y_training, y_testing = np.split(y_all, [int(len(y_all)*(1-test_size))])\n",
    "print(f\"X_training shape (pre-processing): {X_training.shape}\")\n",
    "print(f\"y_training shape (pre-processing): {y_training.shape}\")\n",
    "print(f\"X_testing shape (pre-processing): {X_testing.shape}\")\n",
    "print(f\"y_testing shape (pre-processing): {y_testing.shape}\")\n",
    "\n",
    "# Create a new array where each element is an array of input_size samples in time order\n",
    "# Each element of the new array is shifted by one sample from the previous element\n",
    "indices = np.arange(input_size) + np.arange(len(X_training)-input_size+1)[:,np.newaxis]\n",
    "\n",
    "# We need to split the data otherwise the tf.gather function will throw an error since we run out of memory\n",
    "X_ordered_training = tf.gather(X_training,indices[:len(indices)//2])\n",
    "X_ordered_training = tf.concat([X_ordered_training, tf.gather(X_training,indices[len(indices)//2:])], axis=0)\n",
    "print(f\"X_ordered_training shape: {X_ordered_training.shape}\")\n",
    "\n",
    "indices = np.arange(input_size) + np.arange(len(X_testing)-input_size+1)[:,np.newaxis]\n",
    "X_ordered_testing = tf.gather(X_testing,indices) \n",
    "print(f\"X_ordered_testing shape: {X_ordered_testing.shape}\")\n",
    "\n",
    "# The input size defines the number of samples used for each prediction\n",
    "# Therefore the first output value that we get is at index input_size-1\n",
    "y_ordered_training = y_training[input_size-1:]\n",
    "print(f\"y_ordered_training shape: {y_ordered_training.shape}\")\n",
    "y_ordered_testing = y_testing[input_size-1:]\n",
    "print(f\"y_ordered_testing shape: {y_ordered_testing.shape}\")\n",
    "\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X_ordered_training)) \n",
    "X_random_training = tf.gather(X_ordered_training, shuffled_indices)\n",
    "y_random_training = tf.gather(y_ordered_training, shuffled_indices)\n",
    "print(f\"X_random_training shape (post-processing): {X_random_training.shape}\")\n",
    "print(f\"y_random_training shape (post-processing): {y_random_training.shape}\")\n",
    "\n",
    "print(f\"The X_random_training data is an array, where each element is an array of input_size samples in time order. Therefore the lenght is smaller than the original X_training array (the first {input_size} samples are grouped).\")\n",
    "print(f\"The y_random_training data is an array, where each element is a single sample. This single sample is the target output for the corresponding X_random_training element, which consists of input samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(1, 150, 1)]             0         \n",
      "                                                                 \n",
      " zero_padding1d (ZeroPaddin  (1, 174, 1)               0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (1, 14, 16)               208       \n",
      "                                                                 \n",
      " zero_padding1d_1 (ZeroPadd  (1, 38, 16)               0         \n",
      " ing1D)                                                          \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (1, 3, 16)                3088      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (1, 36)                   7632      \n",
      "                                                                 \n",
      " dense (Dense)               (1, 1)                    37        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10965 (42.83 KB)\n",
      "Trainable params: 10965 (42.83 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''This is a similar Tensorflow/Keras implementation of the LSTM model from the paper:\n",
    "    \"Real-Time Guitar Amplifier Emulation with Deep Learning\"\n",
    "    https://www.mdpi.com/2076-3417/10/3/766/htm\n",
    "\n",
    "    Uses a stack of two 1-D Convolutional layers, followed by LSTM, followed by \n",
    "    a Dense (fully connected) layer. Three preset training modes are available, \n",
    "    with further customization by editing the code. A Functional keras model \n",
    "    is implemented here.\n",
    "\n",
    "    Note: RAM may be a limiting factor for the parameter \"input_size\". The wav data\n",
    "      is preprocessed and stored in RAM, which improves training speed but quickly runs out\n",
    "      if using a large number for \"input_size\".  Reduce this if you are experiencing\n",
    "      RAM issues.\n",
    "'''\n",
    "\n",
    "if train_mode == 0:         # Speed Training\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 12   \n",
    "    conv1d_1_strides = 12\n",
    "    conv1d_filters = 16\n",
    "    hidden_units = 36\n",
    "elif train_mode == 1:       # Accuracy Training (~10x longer than Speed Training)\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 4\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 64\n",
    "else:                       # Extended Training (~60x longer than Accuracy Training)\n",
    "    learning_rate = 0.0005 \n",
    "    conv1d_strides = 3\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 96\n",
    "\n",
    "# Create Functional Model ###########################################\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Define the input shape\n",
    "inputs = keras.Input(shape=(input_size,1), batch_size=1)\n",
    "\n",
    "# The stacked layers\n",
    "x = keras.layers.ZeroPadding1D(padding=12, batch_size=1)(inputs)\n",
    "x = keras.layers.Conv1D(filters=conv1d_filters, kernel_size=12, strides=conv1d_strides, activation=None, batch_size=1)(x)\n",
    "x = keras.layers.ZeroPadding1D(padding=12, batch_size=1)(x)\n",
    "x = keras.layers.Conv1D(filters=conv1d_filters, kernel_size=12, strides=conv1d_strides, activation=None, batch_size=1)(x)\n",
    "x = keras.layers.LSTM(units=hidden_units, activation=keras.activations.tanh, return_sequences=False, stateful=False, batch_size=1, use_bias=True)(x)\n",
    "\n",
    "# The output layer\n",
    "outputs = keras.layers.Dense(units=1, activation=None, batch_size=1)(x)\n",
    "\n",
    "# Create the model and compile\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609/1609 [==============================] - 32s 19ms/step - loss: 7.4732e-04 - val_loss: 2.6125e-04\n",
      "INFO:tensorflow:Assets written to: models/model_0/model_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/model_0/model_0/assets\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_random_training, y=y_random_training, epochs=epochs, batch_size=batch_size, validation_data=(X_ordered_testing, y_ordered_testing)) \n",
    "model.save('models/'+name+'/'+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions\n",
    "### 0. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-24 15:22:18.352771: W tensorflow/core/util/tensor_slice_reader.cc:98] Could not open models/model_0/model_0: FAILED_PRECONDITION: models/model_0/model_0; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x28b776350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('models/'+name+'/'+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. On the test audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-24 10:30:02.169334: W tensorflow/core/util/tensor_slice_reader.cc:98] Could not open models/model_0/model_0: FAILED_PRECONDITION: models/model_0/model_0; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction..\n",
      "51464/51464 [==============================] - 22s 429us/step\n",
      "X_testing shape:  (1646977, 1)\n",
      "X_ordered_testing shape:  (1646828, 150, 1)\n",
      "y_testing shape:  (1646977, 1)\n",
      "prediction shape:  (1646828, 1)\n",
      "Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Prediction #################################################\n",
    "# Test the model on the testing data #############################\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "prediction = model.predict(X_ordered_testing)\n",
    "\n",
    "save_wav('models/'+name+'/y_pred.wav', prediction)\n",
    "save_wav('models/'+name+'/x_test.wav', X_testing)\n",
    "save_wav('models/'+name+'/y_test.wav', y_testing)\n",
    "\n",
    "print(\"X_testing shape: \", X_testing.shape)\n",
    "print(\"X_ordered_testing shape: \", X_ordered_testing.shape)\n",
    "print(\"y_testing shape: \", y_testing.shape)\n",
    "print(\"prediction shape: \", prediction.shape)\n",
    "\n",
    "print(\"Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. On a number sequence (to control inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction..\n",
      "1/1 [==============================] - 0s 314ms/step\n",
      "prediction [[-0.14818978]\n",
      " [-0.216346  ]]\n",
      "X_testing_2 shape:  (2, 150, 1)\n",
      "prediction_2 shape:  (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the model simple number sequence to compare with inference #\n",
    "X_testing_2 = np.array([])\n",
    "\n",
    "for i in range(0, 300):\n",
    "    X_testing_2 = np.append(X_testing_2, i*0.001)\n",
    "\n",
    "X_testing_2 = np.expand_dims(X_testing_2, axis=0)\n",
    "X_testing_2 = np.expand_dims(X_testing_2, axis=0)\n",
    "\n",
    "X_testing_2 = np.reshape(X_testing_2, (2, 150, 1))\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "prediction_2 = model.predict(X_testing_2)\n",
    "print(f\"prediction {prediction_2}\")\n",
    "\n",
    "print(\"X_testing_2 shape: \", X_testing_2.shape)\n",
    "print(\"prediction_2 shape: \", prediction_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as tflite model\n",
    "### 1. for minimal examples (with batch size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_minimal = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n",
      "2023-10-24 11:55:23.027696: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-10-24 11:55:23.027761: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-24 11:55:23.027899: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 11:55:23.027909: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-10-24 11:55:23.154804: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-10-24 11:55:23.154817: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-10-24 11:55:23.169869: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2245] Estimated count of arithmetic ops: 0.102 M  ops, equivalently 0.051 M  MACs\n"
     ]
    }
   ],
   "source": [
    "input_shape = model.inputs[0].shape.as_list()\n",
    "input_shape[0] = batch_size_minimal\n",
    "func = tf.function(model).get_concrete_function(\n",
    "    tf.TensorSpec(input_shape, model.inputs[0].dtype))\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([func])\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(\"models/\"+name+\"/\"+name+\"-minimal.tflite\", 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "# tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf2onnx.tf_loader:Could not search for non-variable resources. Concrete function internal representation may have changed.\n",
      "2023-10-24 11:55:25.322339: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-10-24 11:55:25.322408: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-24 11:55:25.325961: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 11:55:25.325974: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-10-24 11:55:25.584208: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-10-24 11:55:25.584267: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-24 11:55:25.584374: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 11:55:25.584383: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "# Define the input shape\n",
    "input_signature = [tf.TensorSpec([batch_size_minimal, input_size, 1], tf.float32, name='x')]\n",
    "\n",
    "# Convert the model\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
    "onnx.save(proto=onnx_model, f=\"models/\"+name+\"/\"+name+\"-tflite\"+\"-minimal.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. for real-time streaming (with batch size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_streaming = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n",
      "2023-10-24 15:22:31.259267: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-10-24 15:22:31.259339: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-24 15:22:31.260768: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2023-10-24 15:22:31.260791: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-10-24 15:22:31.260795: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-10-24 15:22:31.260854: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 15:22:31.260891: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-10-24 15:22:31.395033: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-10-24 15:22:31.395046: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TFLite ModelAnalyzer ===\n",
      "\n",
      "Your TFLite model has '3' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the PAD op takes\n",
      "tensor #0 and tensor #15 as input and produces tensor #21 as output.\n",
      "\n",
      "Subgraph#0 main(T#0) -> [T#37]\n",
      "  Op#0 PAD(T#0, T#15[0, 0, 12, 12, 0, ...]) -> [T#21]\n",
      "  Op#1 RESHAPE(T#21, T#3[128, 1, 174, 1]) -> [T#22]\n",
      "  Op#2 CONV_2D(T#22, T#7, T#1) -> [T#23]\n",
      "  Op#3 RESHAPE(T#23, T#4[128, 14, 16]) -> [T#24]\n",
      "  Op#4 PAD(T#24, T#15[0, 0, 12, 12, 0, ...]) -> [T#25]\n",
      "  Op#5 RESHAPE(T#25, T#5[128, 1, 38, 16]) -> [T#26]\n",
      "  Op#6 CONV_2D(T#26, T#8, T#2) -> [T#27]\n",
      "  Op#7 RESHAPE(T#27, T#6[128, 3, 16]) -> [T#28]\n",
      "  Op#8 TRANSPOSE(T#28, T#17[1, 0, 2]) -> [T#29]\n",
      "  Op#9 WHILE(T#19[0], T#19[0], T#12, T#13, T#13, T#29, Cond: Subgraph#1, Body: Subgraph#2) -> [T#30, T#31, T#32, T#33, T#34, T#35]\n",
      "  Op#10 STRIDED_SLICE(T#32, T#9[-1, 0, 0], T#10[0, 128, 36], T#11[1, 1, 1]) -> [T#36]\n",
      "  Op#11 FULLY_CONNECTED(T#36, T#20, T#18) -> [T#37]\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(args_0) shape:[128, 150, 1], type:FLOAT32\n",
      "  T#1(model/conv1d/BiasAdd/ReadVariableOp/resource) shape:[16], type:FLOAT32 RO 64 bytes, buffer: 2, data:[-0.00639128, -0.00703551, 0.00228243, -0.0137976, -0.00964908, ...]\n",
      "  T#2(model/conv1d_1/BiasAdd/ReadVariableOp/resource) shape:[16], type:FLOAT32 RO 64 bytes, buffer: 3, data:[0.0183422, 0.0140636, -0.0220616, -0.0199623, -0.0959258, ...]\n",
      "  T#3(model/conv1d/Conv1D/ExpandDims) shape:[4], type:INT32 RO 16 bytes, buffer: 4, data:[128, 1, 174, 1]\n",
      "  T#4(model/conv1d/Conv1D/Squeeze) shape:[3], type:INT32 RO 12 bytes, buffer: 5, data:[128, 14, 16]\n",
      "  T#5(model/conv1d_1/Conv1D/ExpandDims) shape:[4], type:INT32 RO 16 bytes, buffer: 6, data:[128, 1, 38, 16]\n",
      "  T#6(model/conv1d_1/Conv1D/Squeeze) shape:[3], type:INT32 RO 12 bytes, buffer: 7, data:[128, 3, 16]\n",
      "  T#7(model/conv1d/Conv1D) shape:[16, 1, 12, 1], type:FLOAT32 RO 768 bytes, buffer: 8, data:[0.379673, 0.24539, 0.492858, 0.612726, 0.503913, ...]\n",
      "  T#8(model/conv1d_1/Conv1D) shape:[16, 1, 12, 16], type:FLOAT32 RO 12288 bytes, buffer: 9, data:[0.242897, -0.571169, -0.61761, -0.190251, -0.234319, ...]\n",
      "  T#9(strided_slice_2;model/lstm/PartitionedCall/strided_slice_2) shape:[3], type:INT32 RO 12 bytes, buffer: 10, data:[-1, 0, 0]\n",
      "  T#10(strided_slice_2;model/lstm/PartitionedCall/strided_slice_21) shape:[3], type:INT32 RO 12 bytes, buffer: 11, data:[0, 128, 36]\n",
      "  T#11(strided_slice_2;model/lstm/PartitionedCall/strided_slice_22) shape:[3], type:INT32 RO 12 bytes, buffer: 12, data:[1, 1, 1]\n",
      "  T#12(TensorArrayV2_1;model/lstm/PartitionedCall/TensorArrayV2_1) shape:[1, 128, 36], type:FLOAT32 RO 18432 bytes, buffer: 13, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#13(model/lstm/zeros) shape:[128, 36], type:FLOAT32 RO 18432 bytes, buffer: 13, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#14(strided_slice;model/lstm/PartitionedCall/strided_slice) shape:[], type:INT32 RO 4 bytes, buffer: 15, data:[3]\n",
      "  T#15(model/zero_padding1d/Pad/paddings) shape:[3, 2], type:INT32 RO 24 bytes, buffer: 16, data:[0, 0, 12, 12, 0, ...]\n",
      "  T#16(model/lstm/Read_2/ReadVariableOp/resource) shape:[144], type:FLOAT32 RO 576 bytes, buffer: 17, data:[-0.273044, -0.291953, 0.675976, 0.218638, -0.132412, ...]\n",
      "  T#17(transpose/perm;model/lstm/PartitionedCall/transpose/perm) shape:[3], type:INT32 RO 12 bytes, buffer: 18, data:[1, 0, 2]\n",
      "  T#18(model/dense/BiasAdd/ReadVariableOp/resource) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 19, data:[-0.00218528]\n",
      "  T#19(model/conv1d/Conv1D/ExpandDims_1/dim) shape:[], type:INT32 RO 4 bytes, buffer: 20, data:[0]\n",
      "  T#20(model/dense/MatMul) shape:[1, 36], type:FLOAT32 RO 144 bytes, buffer: 21, data:[0.0928863, 0.212152, 0.142203, -0.0439729, 0.0866588, ...]\n",
      "  T#21(model/zero_padding1d/Pad) shape:[128, 174, 1], type:FLOAT32\n",
      "  T#22(model/conv1d/Conv1D/ExpandDims1) shape:[128, 1, 174, 1], type:FLOAT32\n",
      "  T#23(model/conv1d/BiasAdd;model/conv1d/Conv1D/Squeeze;model/conv1d/BiasAdd/ReadVariableOp/resource;model/conv1d_1/Conv1D;model/conv1d/Conv1D) shape:[128, 1, 14, 16], type:FLOAT32\n",
      "  T#24(model/conv1d/BiasAdd;model/conv1d/Conv1D/Squeeze;model/conv1d/BiasAdd/ReadVariableOp/resource) shape:[128, 14, 16], type:FLOAT32\n",
      "  T#25(model/zero_padding1d_1/Pad) shape:[128, 38, 16], type:FLOAT32\n",
      "  T#26(model/conv1d_1/Conv1D/ExpandDims1) shape:[128, 1, 38, 16], type:FLOAT32\n",
      "  T#27(model/conv1d_1/BiasAdd;model/conv1d_1/Conv1D/Squeeze;model/conv1d_1/BiasAdd/ReadVariableOp/resource;model/conv1d_1/Conv1D) shape:[128, 1, 3, 16], type:FLOAT32\n",
      "  T#28(model/conv1d_1/BiasAdd;model/conv1d_1/Conv1D/Squeeze;model/conv1d_1/BiasAdd/ReadVariableOp/resource) shape:[128, 3, 16], type:FLOAT32\n",
      "  T#29(transpose;model/lstm/PartitionedCall/transpose) shape:[3, 128, 16], type:FLOAT32\n",
      "  T#30(while;model/lstm/PartitionedCall/while) shape:[], type:INT32\n",
      "  T#31(while;model/lstm/PartitionedCall/while1) shape:[], type:INT32\n",
      "  T#32(while;model/lstm/PartitionedCall/while2) shape:[1, 128, 36], type:FLOAT32\n",
      "  T#33(while;model/lstm/PartitionedCall/while3) shape:[128, 36], type:FLOAT32\n",
      "  T#34(while;model/lstm/PartitionedCall/while4) shape:[128, 36], type:FLOAT32\n",
      "  T#35(while;model/lstm/PartitionedCall/while5) shape:[3, 128, 16], type:FLOAT32\n",
      "  T#36(strided_slice_2;model/lstm/PartitionedCall/strided_slice_23) shape:[128, 36], type:FLOAT32\n",
      "  T#37(Identity) shape:[128, 1], type:FLOAT32\n",
      "\n",
      "Subgraph#1 while;model/lstm/PartitionedCall/while_cond(T#1_0, T#1_1, T#1_2, T#1_3, T#1_4, T#1_5) -> [T#1_7]\n",
      "  Op#0 LESS(T#1_1, T#1_6[3]) -> [T#1_7]\n",
      "\n",
      "Tensors of Subgraph#1\n",
      "  T#1_0(arg0) shape:[], type:INT32\n",
      "  T#1_1(arg1) shape:[], type:INT32\n",
      "  T#1_2(arg2) shape:[1, 128, 36], type:FLOAT32\n",
      "  T#1_3(arg3) shape:[128, 36], type:FLOAT32\n",
      "  T#1_4(arg4) shape:[128, 36], type:FLOAT32\n",
      "  T#1_5(arg5) shape:[3, 128, 16], type:FLOAT32\n",
      "  T#1_6(strided_slice;model/lstm/PartitionedCall/strided_slice1) shape:[], type:INT32 RO 4 bytes, buffer: 45, data:[3]\n",
      "  T#1_7(while/Less) shape:[], type:BOOL\n",
      "\n",
      "Subgraph#2 while;model/lstm/PartitionedCall/while_body(T#2_0, T#2_1, T#2_2, T#2_3, T#2_4, T#2_5) -> [T#2_31, T#2_11, T#2_30, T#2_29, T#2_27, T#2_5]\n",
      "  Op#0 ADD(T#2_1, T#2_10[1]) -> [T#2_11]\n",
      "  Op#1 FULLY_CONNECTED(T#2_3, T#2_9, T#-1) -> [T#2_12]\n",
      "  Op#2 GATHER(T#2_5, T#2_1) -> [T#2_13]\n",
      "  Op#3 FULLY_CONNECTED(T#2_13, T#2_8, T#-1) -> [T#2_14]\n",
      "  Op#4 ADD(T#2_14, T#2_12) -> [T#2_15]\n",
      "  Op#5 ADD(T#2_15, T#2_6) -> [T#2_16]\n",
      "  Op#6 SPLIT(T#2_10[1], T#2_16) -> [T#2_17, T#2_18, T#2_19, T#2_20]\n",
      "  Op#7 LOGISTIC(T#2_17) -> [T#2_21]\n",
      "  Op#8 LOGISTIC(T#2_18) -> [T#2_22]\n",
      "  Op#9 MUL(T#2_22, T#2_4) -> [T#2_23]\n",
      "  Op#10 LOGISTIC(T#2_20) -> [T#2_24]\n",
      "  Op#11 TANH(T#2_19) -> [T#2_25]\n",
      "  Op#12 MUL(T#2_21, T#2_25) -> [T#2_26]\n",
      "  Op#13 ADD(T#2_23, T#2_26) -> [T#2_27]\n",
      "  Op#14 TANH(T#2_27) -> [T#2_28]\n",
      "  Op#15 MUL(T#2_24, T#2_28) -> [T#2_29]\n",
      "  Op#16 RESHAPE(T#2_29, T#2_7[1, 128, 36]) -> [T#2_30]\n",
      "  Op#17 ADD(T#2_0, T#2_10[1]) -> [T#2_31]\n",
      "\n",
      "Tensors of Subgraph#2\n",
      "  T#2_0(arg0) shape:[], type:INT32\n",
      "  T#2_1(arg1) shape:[], type:INT32\n",
      "  T#2_2(arg2) shape:[1, 128, 36], type:FLOAT32\n",
      "  T#2_3(arg3) shape:[128, 36], type:FLOAT32\n",
      "  T#2_4(arg4) shape:[128, 36], type:FLOAT32\n",
      "  T#2_5(arg5) shape:[3, 128, 16], type:FLOAT32\n",
      "  T#2_6(model/lstm/Read_2/ReadVariableOp/resource1) shape:[144], type:FLOAT32 RO 576 bytes, buffer: 53, data:[-0.273044, -0.291953, 0.675976, 0.218638, -0.132412, ...]\n",
      "  T#2_7(while/TensorArrayV2Write/TensorListSetItem) shape:[3], type:INT32 RO 12 bytes, buffer: 54, data:[1, 128, 36]\n",
      "  T#2_8(while/MatMul) shape:[144, 16], type:FLOAT32 RO 9216 bytes, buffer: 55, data:[-0.00901565, -0.131223, -0.0329599, -0.137585, -0.0901903, ...]\n",
      "  T#2_9(while/MatMul_11) shape:[144, 36], type:FLOAT32 RO 20736 bytes, buffer: 56, data:[-0.262119, -0.323021, 0.0694119, -0.0371232, 0.696546, ...]\n",
      "  T#2_10(while/add_2/y) shape:[], type:INT32 RO 4 bytes, buffer: 57, data:[1]\n",
      "  T#2_11(while/add_2) shape:[], type:INT32\n",
      "  T#2_12(while/MatMul_12) shape:[128, 144], type:FLOAT32\n",
      "  T#2_13(while/TensorArrayV2Read/TensorListGetItem;model/conv1d/Conv1D/ExpandDims_1/dim) shape:[128, 16], type:FLOAT32\n",
      "  T#2_14(while/MatMul1) shape:[128, 144], type:FLOAT32\n",
      "  T#2_15(while/add) shape:[128, 144], type:FLOAT32\n",
      "  T#2_16(while/BiasAdd) shape:[128, 144], type:FLOAT32\n",
      "  T#2_17(while/split) shape:[128, 36], type:FLOAT32\n",
      "  T#2_18(while/split1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_19(while/split2) shape:[128, 36], type:FLOAT32\n",
      "  T#2_20(while/split3) shape:[128, 36], type:FLOAT32\n",
      "  T#2_21(while/Sigmoid) shape:[128, 36], type:FLOAT32\n",
      "  T#2_22(while/Sigmoid_1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_23(while/mul) shape:[128, 36], type:FLOAT32\n",
      "  T#2_24(while/Sigmoid_2) shape:[128, 36], type:FLOAT32\n",
      "  T#2_25(while/Tanh) shape:[128, 36], type:FLOAT32\n",
      "  T#2_26(while/mul_1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_27(while/add_1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_28(while/Tanh_1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_29(while/mul_2) shape:[128, 36], type:FLOAT32\n",
      "  T#2_30(while/TensorArrayV2Write/TensorListSetItem1) shape:[1, 128, 36], type:FLOAT32\n",
      "  T#2_31(while/add_3) shape:[], type:INT32\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:      72688 bytes\n",
      "    Non-data buffer size:       9564 bytes (13.16 %)\n",
      "  Total data buffer size:      63124 bytes (86.84 %)\n",
      "          - Subgraph#0  :      50908 bytes (70.04 %)\n",
      "          - Subgraph#1  :          4 bytes (00.01 %)\n",
      "          - Subgraph#2  :      30544 bytes (42.02 %)\n",
      "    (Zero value buffers):      18436 bytes (25.36 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  (Consider use `converter._experimental_unfold_large_splat_constant` to save the model size.)\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-24 15:22:31.440740: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2245] Estimated count of arithmetic ops: 6.538 M  ops, equivalently 3.269 M  MACs\n"
     ]
    }
   ],
   "source": [
    "input_shape = model.inputs[0].shape.as_list()\n",
    "input_shape[0] = batch_size_streaming\n",
    "func = tf.function(model).get_concrete_function(\n",
    "    tf.TensorSpec(input_shape, model.inputs[0].dtype))\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([func])\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(\"models/\"+name+\"/\"+name+\"-streaming.tflite\", 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf2onnx.tf_loader:Could not search for non-variable resources. Concrete function internal representation may have changed.\n",
      "2023-10-24 15:22:35.191437: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-10-24 15:22:35.191501: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-24 15:22:35.191641: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 15:22:35.191653: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-10-24 15:22:35.402321: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-10-24 15:22:35.402382: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-24 15:22:35.402491: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 15:22:35.402500: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-10-24 15:22:35.464633: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "# Define the input shape\n",
    "input_signature = [tf.TensorSpec([batch_size_streaming, input_size, 1], tf.float32, name='x')]\n",
    "\n",
    "# Convert the model\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
    "onnx.save(proto=onnx_model, f=\"models/\"+name+\"/\"+name+\"-tflite\"+\"-streaming.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping layer: <keras.src.engine.input_layer.InputLayer object at 0x10469b0d0>\n"
     ]
    }
   ],
   "source": [
    "# Save the model as a JSON file (from RTNeural repo) ###################################\n",
    "import model_utils_RTNeural\n",
    "\n",
    "model_utils_RTNeural.save_model(model, filename=\"models/\"+name+\"/\"+name+\".json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
